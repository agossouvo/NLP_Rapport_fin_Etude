{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Corona_NLP_train.csv',encoding ='latin1',error_bad_lines=False)\n",
    "test = pd.read_csv('Corona_NLP_test.csv',encoding ='latin1',error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3804</td>\n",
       "      <td>48756</td>\n",
       "      <td>ÃT: 36.319708,-82.363649</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>As news of the regionÂs first confirmed COVID...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3805</td>\n",
       "      <td>48757</td>\n",
       "      <td>35.926541,-78.753267</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Cashier at grocery store was sharing his insig...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3806</td>\n",
       "      <td>48758</td>\n",
       "      <td>Austria</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Was at the supermarket today. Didn't buy toile...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3807</td>\n",
       "      <td>48759</td>\n",
       "      <td>Atlanta, GA USA</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Due to COVID-19 our retail store and classroom...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3808</td>\n",
       "      <td>48760</td>\n",
       "      <td>BHAVNAGAR,GUJRAT</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>For corona prevention,we should stop to buy th...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3809</td>\n",
       "      <td>48761</td>\n",
       "      <td>Makati, Manila</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>All month there hasn't been crowding in the su...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3810</td>\n",
       "      <td>48762</td>\n",
       "      <td>Pitt Meadows, BC, Canada</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Due to the Covid-19 situation, we have increas...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3811</td>\n",
       "      <td>48763</td>\n",
       "      <td>Horningsea</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>#horningsea is a caring community. LetÂs ALL ...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3812</td>\n",
       "      <td>48764</td>\n",
       "      <td>Chicago, IL</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me: I don't need to stock up on food, I'll jus...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3813</td>\n",
       "      <td>48765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>ADARA Releases COVID-19 Resource Center for Tr...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    UserName  ScreenName                   Location     TweetAt  \\\n",
       "0       3799       48751                     London  16-03-2020   \n",
       "1       3800       48752                         UK  16-03-2020   \n",
       "2       3801       48753                  Vagabonds  16-03-2020   \n",
       "3       3802       48754                        NaN  16-03-2020   \n",
       "4       3803       48755                        NaN  16-03-2020   \n",
       "5       3804       48756  ÃT: 36.319708,-82.363649  16-03-2020   \n",
       "6       3805       48757       35.926541,-78.753267  16-03-2020   \n",
       "7       3806       48758                    Austria  16-03-2020   \n",
       "8       3807       48759            Atlanta, GA USA  16-03-2020   \n",
       "9       3808       48760           BHAVNAGAR,GUJRAT  16-03-2020   \n",
       "10      3809       48761             Makati, Manila  16-03-2020   \n",
       "11      3810       48762  Pitt Meadows, BC, Canada   16-03-2020   \n",
       "12      3811       48763                 Horningsea  16-03-2020   \n",
       "13      3812       48764                Chicago, IL  16-03-2020   \n",
       "14      3813       48765                        NaN  16-03-2020   \n",
       "\n",
       "                                        OriginalTweet           Sentiment  \n",
       "0   @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1   advice Talk to your neighbours family to excha...            Positive  \n",
       "2   Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3   My food stock is not the only one which is emp...            Positive  \n",
       "4   Me, ready to go at supermarket during the #COV...  Extremely Negative  \n",
       "5   As news of the regionÂs first confirmed COVID...            Positive  \n",
       "6   Cashier at grocery store was sharing his insig...            Positive  \n",
       "7   Was at the supermarket today. Didn't buy toile...             Neutral  \n",
       "8   Due to COVID-19 our retail store and classroom...            Positive  \n",
       "9   For corona prevention,we should stop to buy th...            Negative  \n",
       "10  All month there hasn't been crowding in the su...             Neutral  \n",
       "11  Due to the Covid-19 situation, we have increas...  Extremely Positive  \n",
       "12  #horningsea is a caring community. LetÂs ALL ...  Extremely Positive  \n",
       "13  Me: I don't need to stock up on food, I'll jus...            Positive  \n",
       "14  ADARA Releases COVID-19 Resource Center for Tr...            Positive  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/iFz9FAn2Pa and https://t.co/xX6ghGFzCC and https://t.co/I2NlzdxNo8\n",
      "Me, ready to go at supermarket during the #COVID19 outbreak.\r",
      "\r\n",
      "\r",
      "\r\n",
      "Not because I'm paranoid, but because my food stock is litteraly empty. The #coronavirus is a serious thing, but please, don't panic. It causes shortage...\r",
      "\r\n",
      "\r",
      "\r\n",
      "#CoronavirusFrance #restezchezvous #StayAtHome #confinement https://t.co/usmuaLq72n\n"
     ]
    }
   ],
   "source": [
    "print(df['OriginalTweet'][0])\n",
    "print(df['OriginalTweet'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des Urls et codes HTML\n",
    "def remove_urls(df):\n",
    "    url_remove = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_remove.sub(r'', df)\n",
    "df['text_new']= df['OriginalTweet'].apply(lambda x:remove_urls(x))\n",
    "\n",
    "def remove_html(df):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',df)\n",
    "df['OriginalTweet']=df['text_new'].apply(lambda x:remove_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@MeNyrbie @Phil_Gahan @Chrisitv  and  and \n",
      "Me, ready to go at supermarket during the #COVID19 outbreak.\r",
      "\r\n",
      "\r",
      "\r\n",
      "Not because I'm paranoid, but because my food stock is litteraly empty. The #coronavirus is a serious thing, but please, don't panic. It causes shortage...\r",
      "\r\n",
      "\r",
      "\r\n",
      "#CoronavirusFrance #restezchezvous #StayAtHome #confinement \n"
     ]
    }
   ],
   "source": [
    "print(df['OriginalTweet'][0])\n",
    "print(df['OriginalTweet'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des Hastags \n",
    "def remove_mention(x):\n",
    "    text=re.sub(r'@\\w+','',x)\n",
    "    return text\n",
    "df['text_new']=df['OriginalTweet'].apply(lambda x:remove_mention(x))\n",
    "def remove_hash(x):\n",
    "    text=re.sub(r'#\\w+','',x)\n",
    "    return text\n",
    "df['OriginalTweet']=df['text_new'].apply(lambda x:remove_hash(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    and  and \n",
      "Me, ready to go at supermarket during the  outbreak.\r",
      "\r\n",
      "\r",
      "\r\n",
      "Not because I'm paranoid, but because my food stock is litteraly empty. The  is a serious thing, but please, don't panic. It causes shortage...\r",
      "\r\n",
      "\r",
      "\r\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(df['OriginalTweet'][0])\n",
    "print(df['OriginalTweet'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "more_stopwords = ['u', 'im', 'c']\n",
    "stop_words = stop_words + more_stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>text_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td></td>\n",
       "      <td>Neutral</td>\n",
       "      <td>and  and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk neighbours family exchange phone n...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths give elderly...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock one empty...\\r\\r\\n\\r\\r\\nPLEASE, ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready go supermarket  outbreak.\\r\\r\\n\\r\\r\\...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \\\n",
       "0                                                                Neutral   \n",
       "1  advice Talk neighbours family exchange phone n...            Positive   \n",
       "2  Coronavirus Australia: Woolworths give elderly...            Positive   \n",
       "3  My food stock one empty...\\r\\r\\n\\r\\r\\nPLEASE, ...            Positive   \n",
       "4  Me, ready go supermarket  outbreak.\\r\\r\\n\\r\\r\\...  Extremely Negative   \n",
       "\n",
       "                                            text_new  \n",
       "0                                          and  and   \n",
       "1  advice Talk to your neighbours family to excha...  \n",
       "2  Coronavirus Australia: Woolworths to give elde...  \n",
       "3  My food stock is not the only one which is emp...  \n",
       "4  Me, ready to go at supermarket during the #COV...  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['OriginalTweet'] = df['OriginalTweet'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                     \n",
       "1    advice Talk neighbours family exchange phone n...\n",
       "2    Coronavirus Australia: Woolworths give elderly...\n",
       "3    My food stock one empty...\\r\\r\\n\\r\\r\\nPLEASE, ...\n",
       "4    Me, ready go supermarket  outbreak.\\r\\r\\n\\r\\r\\...\n",
       "Name: OriginalTweet, dtype: object"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['OriginalTweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     \n",
      "Me, ready go supermarket  outbreak.\r",
      "\r\n",
      "\r",
      "\r\n",
      "Not I'm paranoid, food stock litteraly empty. The  serious thing, please, panic. It causes shortage...\r",
      "\r\n",
      "\r",
      "\r\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(df['OriginalTweet'][0])\n",
    "print(df['OriginalTweet'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = re.sub(\"[^a-zA-Z]\", \" \", df['OriginalTweet'][4])\n",
    "\n",
    "#keep numbers\n",
    "df_new =  re.sub(\"[^a-zA-Z0-9\\s]+\", \" \",df['OriginalTweet'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams, bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Me',\n",
       " 'ready',\n",
       " 'to',\n",
       " 'go',\n",
       " 'at',\n",
       " 'supermarket',\n",
       " 'during',\n",
       " 'the',\n",
       " 'outbreak',\n",
       " 'Not',\n",
       " 'because',\n",
       " 'I',\n",
       " 'm',\n",
       " 'paranoid',\n",
       " 'but',\n",
       " 'because',\n",
       " 'my',\n",
       " 'food',\n",
       " 'stock',\n",
       " 'is',\n",
       " 'litteraly',\n",
       " 'empty',\n",
       " 'The',\n",
       " 'is',\n",
       " 'a',\n",
       " 'serious',\n",
       " 'thing',\n",
       " 'but',\n",
       " 'please',\n",
       " 'don',\n",
       " 't',\n",
       " 'panic',\n",
       " 'It',\n",
       " 'causes',\n",
       " 'shortage']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens = word_tokenize(df_new)\n",
    "df_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Me', 'ready'), ('ready', 'to'), ('to', 'go'), ('go', 'at'), ('at', 'supermarket'), ('supermarket', 'during'), ('during', 'the'), ('the', 'outbreak'), ('outbreak', 'Not'), ('Not', 'because'), ('because', 'I'), ('I', 'm'), ('m', 'paranoid'), ('paranoid', 'but'), ('but', 'because'), ('because', 'my'), ('my', 'food'), ('food', 'stock'), ('stock', 'is'), ('is', 'litteraly'), ('litteraly', 'empty'), ('empty', 'The'), ('The', 'is'), ('is', 'a'), ('a', 'serious'), ('serious', 'thing'), ('thing', 'but'), ('but', 'please'), ('please', 'don'), ('don', 't'), ('t', 'panic'), ('panic', 'It'), ('It', 'causes'), ('causes', 'shortage')]\n"
     ]
    }
   ],
   "source": [
    "print(list(bigrams(df_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Me', 'ready', 'to'), ('ready', 'to', 'go'), ('to', 'go', 'at'), ('go', 'at', 'supermarket'), ('at', 'supermarket', 'during'), ('supermarket', 'during', 'the'), ('during', 'the', 'outbreak'), ('the', 'outbreak', 'Not'), ('outbreak', 'Not', 'because'), ('Not', 'because', 'I'), ('because', 'I', 'm'), ('I', 'm', 'paranoid'), ('m', 'paranoid', 'but'), ('paranoid', 'but', 'because'), ('but', 'because', 'my'), ('because', 'my', 'food'), ('my', 'food', 'stock'), ('food', 'stock', 'is'), ('stock', 'is', 'litteraly'), ('is', 'litteraly', 'empty'), ('litteraly', 'empty', 'The'), ('empty', 'The', 'is'), ('The', 'is', 'a'), ('is', 'a', 'serious'), ('a', 'serious', 'thing'), ('serious', 'thing', 'but'), ('thing', 'but', 'please'), ('but', 'please', 'don'), ('please', 'don', 't'), ('don', 't', 'panic'), ('t', 'panic', 'It'), ('panic', 'It', 'causes'), ('It', 'causes', 'shortage')]\n"
     ]
    }
   ],
   "source": [
    "print(list(trigrams(df_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_sentence = [w for w in df_tokens if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Me',\n",
       " 'ready',\n",
       " 'go',\n",
       " 'supermarket',\n",
       " 'outbreak',\n",
       " 'Not',\n",
       " 'I',\n",
       " 'paranoid',\n",
       " 'food',\n",
       " 'stock',\n",
       " 'litteraly',\n",
       " 'empty',\n",
       " 'The',\n",
       " 'serious',\n",
       " 'thing',\n",
       " 'please',\n",
       " 'panic',\n",
       " 'It',\n",
       " 'causes',\n",
       " 'shortage']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stemmed_word = []\n",
    "for words in filtered_sentence:\n",
    "    stemmed_word.append(stemmer.stem(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Me',\n",
       " 'readi',\n",
       " 'go',\n",
       " 'supermarket',\n",
       " 'outbreak',\n",
       " 'not',\n",
       " 'I',\n",
       " 'paranoid',\n",
       " 'food',\n",
       " 'stock',\n",
       " 'litterali',\n",
       " 'empti',\n",
       " 'the',\n",
       " 'seriou',\n",
       " 'thing',\n",
       " 'pleas',\n",
       " 'panic',\n",
       " 'It',\n",
       " 'caus',\n",
       " 'shortag']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatised_word =[]\n",
    "for words in filtered_sentence:\n",
    "    lemmatised_word.append(lemmatizer.lemmatize(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Me',\n",
       " 'ready',\n",
       " 'go',\n",
       " 'supermarket',\n",
       " 'outbreak',\n",
       " 'Not',\n",
       " 'I',\n",
       " 'paranoid',\n",
       " 'food',\n",
       " 'stock',\n",
       " 'litteraly',\n",
       " 'empty',\n",
       " 'The',\n",
       " 'serious',\n",
       " 'thing',\n",
       " 'please',\n",
       " 'panic',\n",
       " 'It',\n",
       " 'cause',\n",
       " 'shortage']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatised_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'que', 'aurez', 'eu', 'fûmes', 'as', 'ses', 'une', 'le', 'étées', 'sera', 'pour', 'suis', 'toi', 'étée', 'eusses', 'mon', 'soient', 'eussions', 'lui', 's', 'aurions', 'avaient', 'eut', 'soyez', 'vos', 'fût', 'ils', 'auriez', 'étantes', 'fussent', 'ait', 'les', 'un', 'aie', 'votre', 'il', 'leur', 'avec', 'se', 'ne', 'ayez', 'eus', 'aurons', 'serions', 'de', 'sont', 'ayante', 'fusses', 'eût', 'étants', 'sois', 'soit', 'étant', 'soyons', 'fus', 'es', 'qu', 'serons', 'seraient', 'fûtes', 'avez', 'me', 'ont', 'seront', 'mais', 'aient', 'étés', 'eurent', 'à', 'nous', 'même', 'on', 'eusse', 'fussions', 'eues', 'avait', 'eussent', 'étaient', 'aurai', 't', 'fut', 'ayants', 'fusse', 'seriez', 'avions', 'son', 'au', 'te', 'moi', 'furent', 'en', 'avais', 'ta', 'aux', 'elle', 'pas', 'qui', 'serait', 'étiez', 'je', 'eue', 'eux', 'aurais', 'est', 'vous', 'd', 'ce', 'l', 'serez', 'aviez', 'serais', 'aura', 'êtes', 'c', 'seras', 'et', 'mes', 'notre', 'n', 'du', 'étions', 'ayons', 'aurait', 'la', 'eussiez', 'eûtes', 'm', 'sa', 'y', 'fussiez', 'des', 'sur', 'serai', 'ces', 'auras', 'tu', 'avons', 'sommes', 'ayant', 'été', 'par', 'étais', 'nos', 'était', 'eûmes', 'auraient', 'dans', 'ai', 'aies', 'étante', 'j', 'ton', 'ou', 'ma', 'ayantes', 'tes', 'auront'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words('french'))\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'few', \"it's\", 'too', 'out', 'should', 'so', 'how', 'as', 'weren', 'further', 'theirs', 'own', \"needn't\", \"shouldn't\", 'it', 'down', 'has', 'its', 'until', 'was', 'mightn', 's', 'won', 'other', 'under', 'from', 'only', 'isn', 'same', 'through', \"didn't\", 'why', \"you've\", 'itself', 'yours', 'will', 'can', 've', 'shan', 'we', 'shouldn', 'his', 'during', 'you', 'between', 'after', 'in', 'doesn', 'i', 'than', 'her', 'him', 'if', 'any', 'me', 'do', 'needn', 'their', 'where', 'being', 'wasn', \"couldn't\", 'having', 'here', 'on', \"weren't\", 'were', 'below', 'does', 'herself', 'with', \"she's\", 'mustn', 'some', 'wouldn', 'themselves', \"wouldn't\", 't', \"you'll\", 'then', 'your', 'am', 'or', 'ours', 'to', 'what', 'but', 'who', \"doesn't\", 'above', 're', \"hadn't\", \"you'd\", 'such', 'which', \"don't\", 'those', \"should've\", 'had', 'd', 'are', 'haven', 'this', 'yourselves', 'our', 'because', 'about', 'before', 'up', 'not', 'by', 'myself', 'my', \"you're\", 'against', 'once', 'don', 'these', 'nor', 'didn', 'll', 'doing', 'most', 'couldn', 'when', 'be', 'just', 'm', 'y', 'himself', \"shan't\", 'is', 'for', 'hadn', 'more', 'she', 'o', \"mightn't\", 'ourselves', 'both', \"wasn't\", 'yourself', \"haven't\", 'off', 'at', 'very', \"won't\", \"aren't\", 'and', 'now', 'hasn', 'a', 'into', 'ain', 'each', 'he', 'of', \"isn't\", 'aren', 'no', \"hasn't\", 'again', 'over', 'there', 'them', \"that'll\", 'have', 'they', 'did', 'all', 'ma', 'whom', 'an', 'while', 'hers', 'that', 'the', \"mustn't\", 'been'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
